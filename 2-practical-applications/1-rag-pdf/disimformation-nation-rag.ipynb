{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f9354d",
   "metadata": {},
   "source": [
    "# RAG Pipeline üèõ Zuckerberg Congressional Hearing transcript | LlamaIndex ü¶ô\n",
    "\n",
    "On March 25, 2021 the United States Congress subpoena'd executives from Facebook, Twitter, and Google for a special meeting called \"Disinformation Nation: Social Media's Role in Promoting Extremism and Misinformation\".\n",
    "\n",
    "The transcript is in the data folder\n",
    "\n",
    "1. Extract data using `pdfplumber`\n",
    "2. Convert to `LlamaIndex Documents`\n",
    "3. Generate vector embeddings using `LlamaIndex` and `MiniLM`\n",
    "4. Query! See what we can learn from all this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cab672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (0.14.4)\n",
      "Requirement already satisfied: openai in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (1.109.1)\n",
      "Requirement already satisfied: pdfplumber in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (0.11.7)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pandas in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentence-transformers in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (5.1.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (0.6.1)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.4 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.14.4)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.6.4)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.5.4)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index) (3.9.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (3.13.0)\n",
      "Requirement already satisfied: aiosqlite in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (2025.9.0)\n",
      "Requirement already satisfied: httpx in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (2.8.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (3.5)\n",
      "Requirement already satisfied: numpy in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (4.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.4->llama-index) (2.0.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-core<0.15.0,>=0.14.4->llama-index) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.22.0)\n",
      "Requirement already satisfied: griffe in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (3.1.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.4->llama-index) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.14.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.1.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.4.1)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: filelock in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.3.0)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Requirement already satisfied: joblib in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.4->llama-index) (3.2.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.4->llama-index) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.4->llama-index) (3.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/codyfeda/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading pymupdf-1.26.5-cp39-abi3-macosx_11_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index openai pdfplumber pandas sentence-transformers llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b24de7",
   "metadata": {},
   "source": [
    "#### Here's a sample snippet of the document:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d3cc9",
   "metadata": {},
   "source": [
    "![image](data/transcript-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43b07",
   "metadata": {},
   "source": [
    "## üßπ Step 1: Extract and Clean Data üßº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9839c10",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Line 2Ô∏è‚É£ Numbers 3Ô∏è‚É£\n",
    "\n",
    "Each line is numbered. These numbers are easily ignored by human eyes and periodically very useful. To LLMs they are very 22     harmful to the 23      semantic meaning 24. So we strip these with a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc005eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line_numbers(text: str) -> str:\n",
    "    \"\"\"Remove leading numeric indices from each line.\"\"\"\n",
    "    return re.sub(r\"^\\s*\\d+\\s+\", \"\", text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d20df",
   "metadata": {},
   "source": [
    "#### Speaker Name Extraction üöú\n",
    "\n",
    "The format in the document is this:\n",
    "\n",
    "When a paragraph starts with `\"*Mr. Zuckerberg.\"`, it means that all the folowing paragraphs are Mark Zuckerberg's until the next time you see the speaker signifier. \n",
    "\n",
    "There is one special exception, `*The Chairman` referrs to Frank Pallone, who is the chairman of the committee.\n",
    "\n",
    "This makes it easy enough for us to extract the speaker name and store it in metadata using a regular expression.\n",
    "\n",
    "Here is how we extract the speaker's last name from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker(line: str) -> str:\n",
    "    \"\"\"Extract speaker name from a line, including periods in titles.\"\"\"\n",
    "    line = line.strip()\n",
    "    # Special case: The Chairman\n",
    "    if re.match(r\"^\\*The Chairman\", line, flags=re.IGNORECASE):\n",
    "        return \"Pallone\"\n",
    "\n",
    "    # Lines starting with *\n",
    "    m = re.match(r\"^\\*([A-Za-z\\.\\-']+ [A-Za-z\\.\\-']+)\", line)\n",
    "    if m:\n",
    "        full_name = m.group(1).strip()\n",
    "        # return only the last name and not Mr. or Ms. prefix\n",
    "        return full_name.split()[-1].rstrip('.')\n",
    "    # fallback: no change\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdc4a4",
   "metadata": {},
   "source": [
    "## ü¶ô Step 2: Convert to LlamaIndex documents ü¶ô\n",
    "\n",
    "A `LlamaIndex` Document is the core data unit that LlamaIndex uses for \n",
    "indexing and retrieval. It holds both the raw text content and optional \n",
    "metadata (like speaker attribution, source, page number, or timestamps) to preserve context.\n",
    "\n",
    "Converting the raw text into Document objects allows LlamaIndex to:\n",
    "- Split and preprocess text intelligently (e.g., chunking, cleaning)\n",
    "- Track source attribution for retrieved passages\n",
    "- Embed, index, and query across data types uniformly\n",
    "\n",
    "### üß± Step 2.1: Chunking üß©\n",
    "\n",
    "`Chunking` LLMs can only make sense of so much text at a time. In typical writing, paragraphs are a great way to chunk. If properly written, a paragraph addresses a single topic or idea. With a transcript, it makes sense to chunk by speaker turn.\n",
    "\n",
    "Our documents will look like this:\n",
    "\n",
    "```\n",
    "Document(\n",
    "  metadata: speaker name\n",
    "  text: quote from the transcript\n",
    ")\n",
    "```\n",
    "\n",
    "Here's our helper function for chunking the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803bc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def chunk_transcript(text: str):\n",
    "    \"\"\"Split text into chunks per speaker turn with metadata.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_speaker = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        speaker = extract_speaker(line)\n",
    "        if speaker:\n",
    "            # start a new chunk if current has content\n",
    "            if current_chunk:\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        text=\"\\n\".join(current_chunk).strip(),\n",
    "                        metadata={\"speaker\": current_speaker}\n",
    "                    )\n",
    "                )\n",
    "                current_chunk = []\n",
    "            current_speaker = speaker\n",
    "            # remove speaker marker from text line\n",
    "            \n",
    "            if line.startswith(\"*\"):\n",
    "              # remove leading *Speaker. or *Mr. Speaker:\n",
    "              line = re.sub(r\"^\\*\\s*[^ ]+(?:\\s+[^\\.:\\s]+)*[\\.:]\\s*\", \"\", line)\n",
    "\n",
    "        current_chunk.append(line)\n",
    "\n",
    "    # append last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\n",
    "            Document(\n",
    "                text=\"\\n\".join(current_chunk).strip(),\n",
    "                metadata={\"speaker\": current_speaker}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c6e84",
   "metadata": {},
   "source": [
    "### Step 2.2: Process PDF\n",
    "\n",
    "1. We load the PDF using `pdfplumber` \n",
    "2. Strip page numbers\n",
    "3. Chunk (extract speaker name, create `LlamaIndex` Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a027e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 768 speaker-based chunks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# --- Extract text from PDF ---\n",
    "pdf_path = \"data/disinformation-nation-transcript-20210325.pdf\"\n",
    "all_text = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            text = clean_line_numbers(text)\n",
    "            all_text.append(text)\n",
    "\n",
    "full_text = \"\\n\".join(all_text)\n",
    "\n",
    "# --- Chunk by speaker ---\n",
    "documents = chunk_transcript(full_text)\n",
    "print(f\"Created {len(documents)} speaker-based chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfa3a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speaker': 'Pallone'}\n",
      "Thank you, Chairman Doyle and\n",
      "Schakowsky, for this very important hearing. We are here\n",
      "today because the spread of disinformation and extremism has\n",
      "been growing online, particularly on social media, where\n",
      "there are little to no guardrails in place to stop it.\n",
      "And unfortunately, this disinformation a ...\n",
      "{'speaker': 'Doyle'}\n",
      "The gentleman yields back. The chair now\n",
      "recognizes Ms. Rodgers, the ranking member of the full\n",
      "committee, for five minutes for her opening statement. ...\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage: check first chunk ---\n",
    "print(documents[11].metadata)\n",
    "print(documents[11].text[:300], \"...\")\n",
    "print(documents[12].metadata)\n",
    "print(documents[12].text[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88d57",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Generate Vector Embeddings ü§ñ\n",
    "\n",
    "Vector embeddings allow LLMs to understand our data.\n",
    "\n",
    "You can find more on this at the end of the document if you want. For now, here is what you need to know...\n",
    "\n",
    "`LlamaIndex.VectorStoreIndex` does two things\n",
    "1. Creates searchable Vector index of each document using `all-MiniLM-L6-v2`\n",
    "2. Organizes these embeddings into a structure optimized for semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df0e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# downloads the model from huggingface and caches it\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "hearing_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5864a5",
   "metadata": {},
   "source": [
    "## ‚ùìStep 4: Query our new knowledgebase‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e194847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4Ô∏è‚É£ Query ---\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-5-mini\")\n",
    "query_engine = hearing_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a21a16ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üß† Let me think!\n",
       "\n",
       "A joint remote hearing of the House Subcommittee on Communications and Technology and the Subcommittee on Consumer Protection and Commerce, titled \"Disinformation Nation: Social Media's Role in Promoting Extremism and Misinformation,\" opened with procedural directions and an extended opening statement from the chair.\n",
       "\n",
       "Key points:\n",
       "- Logistics: hearing held by videoconference because of COVID-19; participants must unmute to speak and be visible to be recognized; a 15-minute recess was scheduled around 3:00 PM; documents for the record were to be submitted to designated staff email addresses.\n",
       "- Chair‚Äôs opening: social media platforms were described as having two faces ‚Äî everyday uses (family, friends, quirky videos) alongside persistent extremist, conspiratorial, and misinformation content. The chair argued that platform recommendation and ranking systems steer users toward harmful content.\n",
       "- January 6 and extremism: this was the first appearance of the three witnesses since the January 6 attack. The chair characterized that attack as planned and nourished on social media, with FBI materials and research cited showing use of platforms for planning, recruitment, and coordination.\n",
       "- Misinformation scale: cited research figures were given for large exposures to false information ‚Äî 1.1 billion exposures to election-related misinformation on one platform in the prior year, and an estimated 3.8 billion exposures to COVID-19 disinformation across five countries on that platform.\n",
       "- Public-health impact: with hundreds of thousands of U.S. COVID deaths noted, the chair warned that ongoing vaccine misinformation on social platforms is undermining vaccination efforts and contributing to vaccine refusal.\n",
       "- Procedure note: witnesses are permitted to have counsel present and members were reminded they may mute themselves while conferring.\n",
       "\n",
       "Overall message: the chair framed social media as having enabled widespread misinformation and extremist organizing with real-world harms (including the Capitol attack and public-health consequences), and called out platforms for failing to protect users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query = \"Summarize the hearing\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Let me think!\\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32341f46",
   "metadata": {},
   "source": [
    "#### That looks great! Now let's get more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7589a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zuckerberg emphasized the challenge in identifying hate speech due to the nuanced difference between racist remarks and statements denouncing racism. He mentioned the need to build systems to handle hate speech content in multiple languages globally. Additionally, he acknowledged the difficulty in distinguishing between hateful speech and speech denouncing hate when enforcing hate speech policies.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üß† Let me think about Zuckerberg!\n",
       "\n",
       "Zuckerberg emphasized the challenge in identifying hate speech due to the nuanced difference between racist remarks and statements denouncing racism. He mentioned the need to build systems to handle hate speech content in multiple languages globally. Additionally, he acknowledged the difficulty in distinguishing between hateful speech and speech denouncing hate when enforcing hate speech policies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a filter to retrieve documents by a specific author\n",
    "\n",
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "filters = MetadataFilters(\n",
    "    filters=[ExactMatchFilter(key=\"speaker\", value=\"Zuckerberg\")]\n",
    ")\n",
    "\n",
    "# Create a query engine with the defined filters\n",
    "zuckerberg_query_engine = hearing_index.as_query_engine(filters=filters)\n",
    "\n",
    "# Query the engine\n",
    "zuckerberg_response = zuckerberg_query_engine.query(\"Detail his thoughs on hate speach with direct quotes\")\n",
    "\n",
    "Markdown(f\"### üß† Let me think about Zuckerberg!\\n\\n{zuckerberg_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "56acee01",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'metadata_filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: only retrieve chunks from Zuckerberg\u001b[39;00m\n\u001b[32m      2\u001b[39m speaker_filter = MetadataFilter(\n\u001b[32m      3\u001b[39m     key=\u001b[33m\"\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     value=\u001b[33m\"\u001b[39m\u001b[33mMr. Zuckerberg\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     operator=FilterOperator.EQ\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mfiltered_query_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize Zuckerberg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms responses about privacy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspeaker_filter\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:277\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;129m@wrapt\u001b[39m.decorator\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(func: Callable, instance: Any, args: \u001b[38;5;28mlist\u001b[39m, kwargs: \u001b[38;5;28mdict\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     bound_args = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    279\u001b[39m         actual_class = \u001b[38;5;28mtype\u001b[39m(instance).\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3295\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3294\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3284\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3274\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3275\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3276\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3281\u001b[39m             ),\n\u001b[32m   3282\u001b[39m         )\n\u001b[32m   3283\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3284\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3285\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3286\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'metadata_filters'"
     ]
    }
   ],
   "source": [
    "# Example: only retrieve chunks from Zuckerberg\n",
    "speaker_filter = MetadataFilter(\n",
    "    key=\"speaker\",\n",
    "    value=\"Mr. Zuckerberg\",\n",
    "    operator=FilterOperator.EQ\n",
    ")\n",
    "\n",
    "response = filtered_query_engine.query(\n",
    "    \"Summarize Zuckerberg's responses about privacy\",\n",
    "    metadata_filters=[speaker_filter]\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d5511",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m speaker_filter = MetadataFilter(\n\u001b[32m      5\u001b[39m     key=\u001b[33m\"\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     value=\u001b[33m\"\u001b[39m\u001b[33mMr. Zuckerberg\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     operator=FilterOperator.EQ\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m query_engine = zuckerberg_index.as_query_engine(\n\u001b[32m     11\u001b[39m     llm=llm,\n\u001b[32m     12\u001b[39m     similarity_top_k=\u001b[32m5\u001b[39m,\n\u001b[32m     13\u001b[39m     include_metadata=\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# ensures metadata is available for filtering\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize Zuckerberg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms responses about privacy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspeaker_filter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# metadata filters go here\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:277\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;129m@wrapt\u001b[39m.decorator\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(func: Callable, instance: Any, args: \u001b[38;5;28mlist\u001b[39m, kwargs: \u001b[38;5;28mdict\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     bound_args = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    279\u001b[39m         actual_class = \u001b[38;5;28mtype\u001b[39m(instance).\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3295\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3294\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3214\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3212\u001b[39m     param = \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[32m   3213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mtoo many positional arguments\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param.kind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[32m   3217\u001b[39m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[32m   3218\u001b[39m         \u001b[38;5;66;03m# argument\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilter, FilterOperator\n",
    "\n",
    "# Example: only retrieve chunks from Zuckerberg\n",
    "speaker_filter = MetadataFilter(\n",
    "    key=\"speaker\",\n",
    "    value=\"Mr. Zuckerberg\",\n",
    "    operator=FilterOperator.EQ\n",
    ")\n",
    "\n",
    "query_engine = zuckerberg_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=5,\n",
    "    include_metadata=True  # ensures metadata is available for filtering\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "185e38e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m speaker_filter = MetadataFilter(\n\u001b[32m      2\u001b[39m     key=\u001b[33m\"\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     value=[\u001b[33m\"\u001b[39m\u001b[33mMr. Zuckerberg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe Chair\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m     operator=FilterOperator.IN\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize Zuckerberg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms responses about privacy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspeaker_filter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# metadata filters go here\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/devel/2025/ai-from-scratch/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:277\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;129m@wrapt\u001b[39m.decorator\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(func: Callable, instance: Any, args: \u001b[38;5;28mlist\u001b[39m, kwargs: \u001b[38;5;28mdict\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     bound_args = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    279\u001b[39m         actual_class = \u001b[38;5;28mtype\u001b[39m(instance).\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3295\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3294\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:3214\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3212\u001b[39m     param = \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[32m   3213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mtoo many positional arguments\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param.kind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[32m   3217\u001b[39m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[32m   3218\u001b[39m         \u001b[38;5;66;03m# argument\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: too many positional arguments"
     ]
    }
   ],
   "source": [
    "speaker_filter = MetadataFilter(\n",
    "    key=\"speaker\",\n",
    "    value=[\"Mr. Zuckerberg\", \"The Chair\"],\n",
    "    operator=FilterOperator.IN\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Summarize Zuckerberg's responses about privacy\",\n",
    "    [speaker_filter]  # metadata filters go here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f0817",
   "metadata": {},
   "source": [
    "## üõë Try that again üîÑ\n",
    "\n",
    "The numbers I spot checked were accurate, but 147,000 million is hard to understand. Let's have it convert it to billions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e91b707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üß† Sales Numbers\n",
       "\n",
       "Three months ended June 30 (Q2)\n",
       "- North America: $90.033 bn ‚Üí $100.068 bn; +$10.035 bn (+11.2%)\n",
       "- International: $31.663 bn ‚Üí $36.761 bn; +$5.098 bn (+16.1%)\n",
       "- AWS: $26.281 bn ‚Üí $30.873 bn; +$4.592 bn (+17.5%)\n",
       "- Consolidated: $147.977 bn ‚Üí $167.702 bn; +$19.725 bn (+13.3%)\n",
       "\n",
       "Net sales by type (Q2)\n",
       "- Net product sales: $61.569 bn ‚Üí $68.246 bn; +$6.677 bn (+10.8%)\n",
       "- Net service sales: $86.408 bn ‚Üí $99.456 bn; +$13.048 bn (+15.1%)\n",
       "\n",
       "Six months ended June 30\n",
       "- North America: $176.374 bn ‚Üí $192.955 bn; +$16.581 bn (+9.4%)\n",
       "- International: $63.598 bn ‚Üí $70.274 bn; +$6.676 bn (+10.5%)\n",
       "- AWS: $51.318 bn ‚Üí $60.140 bn; +$8.822 bn (+17.2%)\n",
       "- Consolidated: $291.290 bn ‚Üí $323.369 bn; +$32.079 bn (+11.0%)\n",
       "\n",
       "Net sales by type (six months)\n",
       "- Net product sales: $122.484 bn ‚Üí $132.216 bn; +$9.732 bn (+7.9%)\n",
       "- Net service sales: $168.806 bn ‚Üí $191.153 bn; +$22.347 bn (+13.2%)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query = \"Breakdown the sales numbers changes for me. Use billions instead of millions.\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Sales Numbers\\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6a5e4",
   "metadata": {},
   "source": [
    "# üèÅüèÅüèÅ We're done! üèÅüèÅüèÅ\n",
    "\n",
    "This was impossible to do just a matter of months ago. If you downgrade to gpt-4o-mini, you get a virtually unusable output. All of the numbers I spot checked were accurate.\n",
    "\n",
    "The paper path that lead us here:\n",
    "\n",
    "| Date | Paper | Lab | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| June 2017 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Google | Transformers paper. This is built the foundation for LLMs |\n",
    "| October 2018 | [Bidirectional encoder representations from transformers (BERT)](https://arxiv.org/abs/1810.04805) | Google | Uses transformer architecture to derive semantic meaning of text chunks |\n",
    "| August 2019 | [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) | UKPLab | Reduces the computation time of BERT from 65 hours to 5 seconds. The `all-MiniLM-L6-v2` we used came from this |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753365b",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ Package notes üì¶\n",
    "\n",
    "`llama-index`\n",
    "- Open Source project to help with the RAG Pipeline\n",
    "\n",
    "`llama-index-embeddings-huggingface`\n",
    "- creates vector embeddings of text\n",
    "- wraps embedding model (in this case... `sentence-transformers/all-MiniLM-L6-v2`)\n",
    "\n",
    "`pdfplumber `\n",
    "- https://github.com/jsvine/pdfplumber \n",
    "- built by data journalist, jsvine. \n",
    "- Extends upon pdfminer parsing engine\n",
    "\n",
    "`pandas`\n",
    "- Data analysis tool\n",
    "- Handles structured and labeled data\n",
    "\n",
    "`sentence-transformers`\n",
    "- UKPLab (Ubiquitous Knowledge Processing Lab) at TU Darmstadt, Germany\n",
    "- Turns sentences (or paragraphs or pages) into embeddings\n",
    "- the model `all-MiniLM-L6-v` does this incredibly fast, read the SentenceBERT Paper above to see how\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ab574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
