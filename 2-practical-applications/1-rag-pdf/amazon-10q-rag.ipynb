{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f9354d",
   "metadata": {},
   "source": [
    "# ü¶ô RAG Pipeline for PDF Analysis using LlamaIndex ü¶ô\n",
    "\n",
    "In the data folder we have Amazon's quarterly financial statement. Here's how we're going to pull that into our knowledgebase\n",
    "\n",
    "1. Extract data using `pdfplumber`\n",
    "2. Convert to `LlamaIndex Documents`\n",
    "3. Generate vector embeddings using `LlamaIndex` and `MiniLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cab672",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index openai pdfplumber pandas sentence-transformers llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43b07",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Extract data from PDF üìÑ\n",
    "\n",
    "`pdfplumber` and `pandas` make quick work of this. \n",
    "\n",
    "What‚Äôs impressive about `pdfplumber` is how it extracts tables and converts them into clean `Pandas DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca70f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 104 text blocks from PDF.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- 1Ô∏è‚É£ Extract from PDF ---\n",
    "pdf_path = \"data/amazon-10q-2025-q2.pdf\"\n",
    "\n",
    "texts = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # Extract plain text\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            texts.append(text.strip())\n",
    "\n",
    "        # Extract tables\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            # Turn table into a readable string\n",
    "            table_str = df.to_string(index=False, header=False)\n",
    "            texts.append(\"Extracted table:\\n\" + table_str)\n",
    "\n",
    "print(f\"Extracted {len(texts)} text blocks from PDF.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdc4a4",
   "metadata": {},
   "source": [
    "This `texts` list contains an element for each page and each table\n",
    "\n",
    "*Note: if the table format isn't clean, you may see single table rows parsed into their own object*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fe6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE THIS DATA FOR YOURSELF\n",
    "\n",
    "###################\n",
    "#PAGE 7 OF THE PDF IS THE FIRST PAGE WITH ONLY TEXT\n",
    "#-------------------\n",
    "#print(texts[10])\n",
    "\n",
    "\n",
    "###################\n",
    "# THIS IS THE EXTRACTED TABLE FROM PDF PAGE 3\n",
    "#-------------------\n",
    "#print(texts[3])\n",
    "\n",
    "\n",
    "###################\n",
    "# OR SEE ALL OF THEM\n",
    "#-------------------\n",
    "# for index, t in enumerate(texts):\n",
    "#   print(f\"{index}---------\")\n",
    "#   print(t[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22ea5c",
   "metadata": {},
   "source": [
    "## ü¶ô Step 2: Convert to LlamaIndex documents ü¶ô\n",
    "\n",
    "A `LlamaIndex` Document is the core data unit that LlamaIndex uses for \n",
    "indexing and retrieval. It holds both the raw text content and optional \n",
    "metadata (like source, page number, or timestamps) to preserve context.\n",
    "\n",
    "Converting the raw text into Document objects allows LlamaIndex to:\n",
    "- Split and preprocess text intelligently (e.g., chunking, cleaning)\n",
    "- Track source attribution for retrieved passages\n",
    "- Embed, index, and query across data types uniformly\n",
    "\n",
    "In this case, each page of text and each table extracted from the PDF \n",
    "becomes its own Document, forming a mini knowledge base for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t) for t in texts if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88d57",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Generate Vector Embeddings ü§ñ\n",
    "\n",
    "Vector embeddings allow LLMs to understand our data.\n",
    "\n",
    "You can find more on this at the end of the document if you want. For now, here is what you need to know...\n",
    "\n",
    "`LlamaIndex.VectorStoreIndex` does two things\n",
    "1. Creates searchable Vector index of each document using `all-MiniLM-L6-v2`\n",
    "2. Organizes these embeddings into a structure optimized for semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# downloads the model from huggingface and caches it\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "amazon_10q_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5864a5",
   "metadata": {},
   "source": [
    "## ‚ùìStep 4: Query our new knowledgebase‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e194847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4Ô∏è‚É£ Query ---\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-5-mini\")\n",
    "query_engine = amazon_10q_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a16ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üß† Sales Numbers\n",
       "\n",
       "Here‚Äôs a concise breakdown of the sales numbers (amounts in millions):\n",
       "\n",
       "Net sales by product/service group\n",
       "- Three months ended June 30\n",
       "  - Online stores: $55,392 (2024) ‚Üí $61,485 (2025)\n",
       "  - Physical stores: $5,206 ‚Üí $5,595\n",
       "  - Third‚Äëparty seller services: $36,201 ‚Üí $40,348\n",
       "  - Advertising services: $12,771 ‚Üí $15,694\n",
       "  - Subscription services: $10,866 ‚Üí $12,208\n",
       "  - AWS: $26,281 ‚Üí $30,873\n",
       "  - Other: $1,260 ‚Üí $1,499\n",
       "  - Consolidated total: $147,977 ‚Üí $167,702\n",
       "\n",
       "- Six months ended June 30\n",
       "  - Online stores: $110,062 ‚Üí $118,892\n",
       "  - Physical stores: $10,408 ‚Üí $11,128\n",
       "  - Third‚Äëparty seller services: $70,797 ‚Üí $76,860\n",
       "  - Advertising services: $24,595 ‚Üí $29,615\n",
       "  - Subscription services: $21,588 ‚Üí $23,923\n",
       "  - AWS: $51,318 ‚Üí $60,140\n",
       "  - Other: $2,522 ‚Üí $2,811\n",
       "  - Consolidated total: $291,290 ‚Üí $323,369\n",
       "\n",
       "Net sales by operating segment\n",
       "- Three months ended June 30\n",
       "  - North America: $90,033 ‚Üí $100,068\n",
       "  - International: $31,663 ‚Üí $36,761\n",
       "  - AWS: $26,281 ‚Üí $30,873\n",
       "  - Consolidated total: $147,977 ‚Üí $167,702\n",
       "\n",
       "- Six months ended June 30\n",
       "  - North America: $176,374 ‚Üí $192,955\n",
       "  - International: $63,598 ‚Üí $70,274\n",
       "  - AWS: $51,318 ‚Üí $60,140\n",
       "  - Consolidated total: $291,290 ‚Üí $323,369\n",
       "\n",
       "Growth and mix highlights\n",
       "- Consolidated year‚Äëover‚Äëyear growth: +13% (Q2 2025) and +11% (six months 2025).\n",
       "- Segment growth (year over year): North America +11% (Q2) and +9% (six months); International +16% (Q2) and +10% (six months); AWS +17% (Q2) and +17% (six months).\n",
       "- Q2 2025 net sales mix: North America 60%, International 22%, AWS 18%.\n",
       "\n",
       "Foreign exchange impact\n",
       "- Foreign exchange increased consolidated net sales by $1.5 billion for Q2 2025; it did not have a significant impact on the six‚Äëmonth period.\n",
       "- Foreign exchange reduced North America net sales by $169 million in Q2 2025 and by $582 million for the six months ended June 30, 2025.\n",
       "\n",
       "If you want, I can reformat this into a table, show percentage changes by product group, or highlight the largest contributors to growth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query = \"Breakdown the sales numbers for me.\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Sales Numbers\\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f0817",
   "metadata": {},
   "source": [
    "## üõë Try that again üîÑ\n",
    "\n",
    "The numbers I spot checked were accurate, but 147,000 million is hard to understand. Let's have it convert it to billions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e91b707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üß† Sales Numbers\n",
       "\n",
       "Three months ended June 30 (Q2)\n",
       "- North America: $90.033 bn ‚Üí $100.068 bn; +$10.035 bn (+11.2%)\n",
       "- International: $31.663 bn ‚Üí $36.761 bn; +$5.098 bn (+16.1%)\n",
       "- AWS: $26.281 bn ‚Üí $30.873 bn; +$4.592 bn (+17.5%)\n",
       "- Consolidated: $147.977 bn ‚Üí $167.702 bn; +$19.725 bn (+13.3%)\n",
       "\n",
       "Net sales by type (Q2)\n",
       "- Net product sales: $61.569 bn ‚Üí $68.246 bn; +$6.677 bn (+10.8%)\n",
       "- Net service sales: $86.408 bn ‚Üí $99.456 bn; +$13.048 bn (+15.1%)\n",
       "\n",
       "Six months ended June 30\n",
       "- North America: $176.374 bn ‚Üí $192.955 bn; +$16.581 bn (+9.4%)\n",
       "- International: $63.598 bn ‚Üí $70.274 bn; +$6.676 bn (+10.5%)\n",
       "- AWS: $51.318 bn ‚Üí $60.140 bn; +$8.822 bn (+17.2%)\n",
       "- Consolidated: $291.290 bn ‚Üí $323.369 bn; +$32.079 bn (+11.0%)\n",
       "\n",
       "Net sales by type (six months)\n",
       "- Net product sales: $122.484 bn ‚Üí $132.216 bn; +$9.732 bn (+7.9%)\n",
       "- Net service sales: $168.806 bn ‚Üí $191.153 bn; +$22.347 bn (+13.2%)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query = \"Breakdown the sales numbers changes for me. Use billions instead of millions.\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Sales Numbers\\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6a5e4",
   "metadata": {},
   "source": [
    "# üèÅüèÅüèÅ We're done! üèÅüèÅüèÅ\n",
    "\n",
    "This was impossible to do just a matter of months ago. If you downgrade to gpt-4o-mini, you get a virtually unusable output. All of the numbers I spot checked were accurate.\n",
    "\n",
    "The paper path that lead us here:\n",
    "\n",
    "| Date | Paper | Lab | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| June 2017 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Google | Transformers paper. This is built the foundation for LLMs |\n",
    "| October 2018 | [Bidirectional encoder representations from transformers (BERT)](https://arxiv.org/abs/1810.04805) | Google | Uses transformer architecture to derive semantic meaning of text chunks |\n",
    "| August 2019 | [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) | UKPLab | Reduces the computation time of BERT from 65 hours to 5 seconds. The `all-MiniLM-L6-v2` we used came from this |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753365b",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ Package notes üì¶\n",
    "\n",
    "llama-index-embeddings-huggingface\n",
    "- creates vector embeddings of text\n",
    "- wraps embedding model (in this case... sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "pdfplumber \n",
    "- https://github.com/jsvine/pdfplumber \n",
    "- built by data journalist, jsvine. \n",
    "- Extends upon pdfminer parsing engine\n",
    "\n",
    "pandas\n",
    "- Data analysis tool\n",
    "- Handles structured and labeled data\n",
    "\n",
    "\n",
    "sentence-transformers\n",
    "- UKPLab (Ubiquitous Knowledge Processing Lab) at TU Darmstadt, Germany\n",
    "- built on top of huggingface transformers\n",
    "- specifically to turn sentences (or paragraphs or pages) into embeddings\n",
    "- utilized with SDET\n",
    "\n",
    "all-MiniLM-L6-v\n",
    "- created by UKPLab (Ubiquitous Knowledge Processing Lab) at TU Darmstadt, Germany\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ab574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
