{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f9354d",
   "metadata": {},
   "source": [
    "# üö¢ RAG Pipeline for PDF Analysis using LlamaIndex ü¶ô\n",
    "\n",
    "*June 18, 2023:*\n",
    "The Titan Submarine began a journey to the Titanic wreckage, it would never resurface.\n",
    "\n",
    "*October 2, 2025:*\n",
    "The National Transit Safety Board published this report of their investigation.\n",
    "\n",
    "Source: https://www.ntsb.gov/investigations/AccidentReports/Reports/MIR2536.pdf\n",
    "\n",
    "We analyze this PDF report in 3 steps:\n",
    "1. Extract data using `pdfplumber`\n",
    "2. Convert to `LlamaIndex Documents`\n",
    "3. Generate vector embeddings using `LlamaIndex` and `MiniLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cab672",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index openai pdfplumber pandas sentence-transformers llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43b07",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Extract data from PDF üìÑ\n",
    "\n",
    "`pdfplumber` and `pandas` make quick work of this. \n",
    "\n",
    "What‚Äôs impressive about `pdfplumber` is how it extracts tables and converts them into clean `Pandas DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca70f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- 1Ô∏è‚É£ Extract from PDF ---\n",
    "pdf_path = \"data/ntsb-titan-submarine-report.pdf\"\n",
    "\n",
    "texts = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # Extract plain text\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            texts.append(text.strip())\n",
    "\n",
    "        # Extract tables\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            df = pd.DataFrame(table)\n",
    "            # Turn table into a readable string\n",
    "            table_str = df.to_string(index=False, header=False)\n",
    "            texts.append(\"Extracted table:\\n\" + table_str)\n",
    "\n",
    "print(f\"Extracted {len(texts)} text blocks from PDF.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdc4a4",
   "metadata": {},
   "source": [
    "This `texts` list contains an element for each page and each table\n",
    "\n",
    "*Note: if the table format isn't clean, you may see single table rows parsed into their own object*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE THIS DATA FOR YOURSELF\n",
    "\n",
    "###################\n",
    "#PAGE 1\n",
    "#-------------------\n",
    "print(\"------ TITLE PAGE -------\")\n",
    "print(texts[0][:300])\n",
    "\n",
    "\n",
    "###################\n",
    "# THIS IS THE ACRONYM AND ABBREVIATION TABLE\n",
    "# (notice each table row is it's own text block)\n",
    "#-------------------\n",
    "print(\"------ PAGE 5 ACRONYM TABLE -------\")\n",
    "print('\\n'.join(texts[6:12]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22ea5c",
   "metadata": {},
   "source": [
    "## ü¶ô Step 2: Convert to LlamaIndex documents ü¶ô\n",
    "\n",
    "A `LlamaIndex` Document is the core data unit that LlamaIndex uses for \n",
    "indexing and retrieval. It holds both the raw text content and optional \n",
    "metadata (like source, page number, or timestamps) to preserve context.\n",
    "\n",
    "Converting the raw text into Document objects allows LlamaIndex to:\n",
    "- Split and preprocess text intelligently (e.g., chunking, cleaning)\n",
    "- Track source attribution for retrieved passages\n",
    "- Embed, index, and query across data types uniformly\n",
    "\n",
    "In this case, each page of text and each table extracted from the PDF \n",
    "becomes its own Document, forming a mini knowledge base for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t) for t in texts if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88d57",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Generate Vector Embeddings ü§ñ\n",
    "\n",
    "Vector embeddings allow LLMs to understand our data.\n",
    "\n",
    "You can find more on this at the end of the document if you want. For now, here is what you need to know...\n",
    "\n",
    "`LlamaIndex.VectorStoreIndex` does two things\n",
    "1. Creates searchable Vector index of each document using `all-MiniLM-L6-v2`\n",
    "2. Organizes these embeddings into a structure optimized for semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# downloads the model from huggingface and caches it\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "titan_report_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5864a5",
   "metadata": {},
   "source": [
    "## ‚ùìStep 4: Query our new knowledgebase‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e194847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4Ô∏è‚É£ Query ---\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-5-mini\")\n",
    "query_engine = titan_report_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "query = \"What could have prevented this tragety?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Response\\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239b54b",
   "metadata": {},
   "source": [
    "### ‚ùìStep 4.1: Now try yours!‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "Markdown(f\"### üß† Submarine answer: \\n\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6a5e4",
   "metadata": {},
   "source": [
    "# üèÅüèÅüèÅ We're done! üèÅüèÅüèÅ\n",
    "\n",
    "The paper path that lead us here:\n",
    "\n",
    "| Date | Paper | Lab | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| June 2017 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Google | Transformers paper. This is built the foundation for LLMs |\n",
    "| October 2018 | [Bidirectional encoder representations from transformers (BERT)](https://arxiv.org/abs/1810.04805) | Google | Uses transformer architecture to derive semantic meaning of text chunks |\n",
    "| August 2019 | [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) | UKPLab | Reduces the computation time of BERT from 65 hours to 5 seconds. The `all-MiniLM-L6-v2` we used came from this |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753365b",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ Package notes üì¶\n",
    "\n",
    "`llama-index`\n",
    "- Open Source project to help with the RAG Pipeline\n",
    "\n",
    "`llama-index-embeddings-huggingface`\n",
    "- creates vector embeddings of text\n",
    "- wraps embedding model (in this case... `sentence-transformers/all-MiniLM-L6-v2`)\n",
    "\n",
    "`pdfplumber `\n",
    "- https://github.com/jsvine/pdfplumber \n",
    "- built by data journalist, jsvine. \n",
    "- Extends upon pdfminer parsing engine\n",
    "\n",
    "`pandas`\n",
    "- Data analysis tool\n",
    "- Handles structured and labeled data\n",
    "\n",
    "`sentence-transformers`\n",
    "- UKPLab (Ubiquitous Knowledge Processing Lab) at TU Darmstadt, Germany\n",
    "- Turns sentences (or paragraphs or pages) into embeddings\n",
    "- the model `all-MiniLM-L6-v` does this incredibly fast, read the SentenceBERT Paper above to see how\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ab574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
